{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "IXbTCGN6Szhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "pFu4PbwguzYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPROCESSING"
      ],
      "metadata": {
        "id": "GEzA17yRuycP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", local_files_only = True)\n",
        "df = (pd.read_json('CADD.json')).transpose()\n",
        "\n",
        "df = df[df[\"comment\"].apply(lambda row: len(tokenizer.tokenize(row))) <= 512]\n",
        "df['label'] = df.apply(lambda row: 0 if (row[\"L1\"] == \"0\") and (row[\"L2\"] == \"0\") and (row[\"L3\"] == \"0\") and (row[\"L4\"] == \"0\") and (row[\"L5\"] == \"0\") and (row[\"L6\"] == \"0\") else 1, axis = 1)\n",
        "\n",
        "df = df.rename(columns = {\"comment\": \"text\"})\n",
        "df = df.drop(columns = [\"title\", \"body\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\"])"
      ],
      "metadata": {
        "id": "_QpMLLqtvFop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_abusive0 = df[(df[\"label\"] == 0)]\n",
        "df_abusive0 = df_abusive0.sample(n = 5000)\n",
        "df_abusive0_train = df_abusive0.sample(n = (int(0.8 * len(df_abusive0))))\n",
        "df_abusive0_eval = df_abusive0[~df_abusive0.index.isin(df_abusive0_train.index)]\n",
        "\n",
        "df_abusive1 = df[(df[\"label\"] == 1)]\n",
        "df_abusive1 = df_abusive1.sample(n = 5000)\n",
        "df_abusive1_train = df_abusive1.sample(n = (int(0.8 * len(df_abusive1))))\n",
        "df_abusive1_eval = df_abusive1[~df_abusive1.index.isin(df_abusive1_train.index)]"
      ],
      "metadata": {
        "id": "oJ8tZXRlBdYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df_abusive0_train, df_abusive1_train])\n",
        "df_train = df_train.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "df_eval = pd.concat([df_abusive0_eval, df_abusive1_eval])\n",
        "df_eval = df_eval.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "torch.save(df_train, 'df_train.pt')\n",
        "torch.save(df_eval, 'df_eval.pt')"
      ],
      "metadata": {
        "id": "285Ds7pCfZEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", local_files_only = True)\n",
        "\n",
        "df_train = torch.load('df_train.pt')\n",
        "df_eval = torch.load('df_eval.pt')\n",
        "\n",
        "class CADD(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.text = []\n",
        "        self.labels = []\n",
        "        self.tokens = []\n",
        "        for i in range(len(df)):\n",
        "            self.text.append(df[\"text\"][i])\n",
        "            self.labels.append(df[\"label\"][i])\n",
        "            self.tokens.append(tokenizer(df[\"text\"][i]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sample = {\"input_ids\": torch.tensor(self.tokens[item][\"input_ids\"]), \"attention_mask\": torch.tensor(self.tokens[item][\"attention_mask\"]), \"labels\": torch.tensor(self.labels[item])}\n",
        "        return sample\n",
        "\n",
        "dataset_train = CADD(df_train)\n",
        "dataset_eval = CADD(df_eval)"
      ],
      "metadata": {
        "id": "4OAUF4QkxmWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FINE-TUNING"
      ],
      "metadata": {
        "id": "qmV1RBNTJoi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", local_files_only = True)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", local_files_only = True)\n",
        "\n",
        "args = TrainingArguments(output_dir = \"model\",\n",
        "                         overwrite_output_dir = True,\n",
        "                         per_device_train_batch_size = 8,\n",
        "                         per_device_eval_batch_size = 1,\n",
        "                         num_train_epochs = 1,\n",
        "                         learning_rate= 0.0001,\n",
        "                         weight_decay = 0.1,\n",
        "                         save_strategy = \"no\",\n",
        "                         logging_strategy = \"epoch\",\n",
        "                         evaluation_strategy = \"epoch\")\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  tokenizer = tokenizer,\n",
        "                  args = args,\n",
        "                  train_dataset = dataset_train,\n",
        "                  eval_dataset = dataset_eval)"
      ],
      "metadata": {
        "id": "j4M7P-fVfLE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "m15etoUsfMJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE"
      ],
      "metadata": {
        "id": "Ig7UeRaWJvmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"model\", local_files_only = True)\n",
        "model = BertForSequenceClassification.from_pretrained(\"model\", local_files_only = True)"
      ],
      "metadata": {
        "id": "3xhIak9CJxrm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"He's an engineer, he solves practical problems.\", return_tensors = \"pt\")\n",
        "\n",
        "outputs = model(inputs)\n",
        "outputs_normalized = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
        "\n",
        "outputs_normalized"
      ],
      "metadata": {
        "id": "41tuufSkJ1Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f71c2a-ffc7-482a-8b5b-49392ae8a258"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9817, 0.0183]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"He's a serial killer, he kills women and children for mere pleasure.\", return_tensors = \"pt\")\n",
        "\n",
        "outputs = model(inputs)\n",
        "outputs_normalized = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
        "\n",
        "outputs_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kys4soRNlpwH",
        "outputId": "f599dec1-8159-47d9-cee0-58cf57dd0542"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0911, 0.9089]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}