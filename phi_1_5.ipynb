{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, PhiForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live"
      ],
      "metadata": {
        "id": "IXbTCGN6Szhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "pFu4PbwguzYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPROCESSING"
      ],
      "metadata": {
        "id": "GEzA17yRuycP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"phi-1_5\", local_files_only = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "aurPSlOlgJNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_samples(genre):\n",
        "    list_ = []\n",
        "    for filename in os.listdir(f\"books/{genre}\"):\n",
        "        with open(f\"books/{genre}/{filename}\", \"r\") as file:\n",
        "            file_content = file.read()\n",
        "        tokenized = tokenizer(file_content)\n",
        "\n",
        "        for i in range(0, len(tokenized[\"input_ids\"]), 2048):\n",
        "            input_ids = tokenized[\"input_ids\"][i:(i + 2048)]\n",
        "            attention_mask = tokenized[\"attention_mask\"][i:(i + 2048)]\n",
        "            list_.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "    return list_\n",
        "\n",
        "df_genreA = collect_samples(\"fantasy\")\n",
        "df_genreB = collect_samples(\"romance\")\n",
        "df_genreC = collect_samples(\"sci-fi\")\n",
        "df_genreD = collect_samples(\"thriller\")"
      ],
      "metadata": {
        "id": "kLd5f4NfabEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_list_to_dataframe(list_):\n",
        "    df = pd.DataFrame(list_, columns = [\"text\"])\n",
        "\n",
        "    for i in range(len(list_)):\n",
        "        df.loc[i, \"text\"] = \"\"\n",
        "        df.at[i, 'text'] = list_[i]\n",
        "    return df\n",
        "\n",
        "df_genreA = convert_list_to_dataframe(df_genreA)\n",
        "df_genreB = convert_list_to_dataframe(df_genreB)\n",
        "df_genreC = convert_list_to_dataframe(df_genreC)\n",
        "df_genreD = convert_list_to_dataframe(df_genreD)"
      ],
      "metadata": {
        "id": "rWhleRIUBD1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_genreA = df_genreA.sample(n = 2500)\n",
        "df_genreA_train = df_genreA.sample(n = (int(0.8 * len(df_genreA))))\n",
        "df_genreA_eval = df_genreA[~df_genreA.index.isin(df_genreA_train.index)]\n",
        "\n",
        "df_genreB = df_genreB.sample(n = 2500)\n",
        "df_genreB_train = df_genreB.sample(n = (int(0.8 * len(df_genreB))))\n",
        "df_genreB_eval = df_genreB[~df_genreB.index.isin(df_genreB_train.index)]\n",
        "\n",
        "df_genreC = df_genreC.sample(n = 2500)\n",
        "df_genreC_train = df_genreC.sample(n = (int(0.8 * len(df_genreC))))\n",
        "df_genreC_eval = df_genreC[~df_genreC.index.isin(df_genreC_train.index)]\n",
        "\n",
        "df_genreD = df_genreD.sample(n = 2500)\n",
        "df_genreD_train = df_genreD.sample(n = (int(0.8 * len(df_genreD))))\n",
        "df_genreD_eval = df_genreD[~df_genreD.index.isin(df_genreD_train.index)]"
      ],
      "metadata": {
        "id": "DT_yGu3_p1zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df_genreA_train, df_genreB_train, df_genreC_train, df_genreD_train])\n",
        "df_train = df_train.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "df_eval = pd.concat([df_genreA_eval, df_genreB_eval, df_genreC_eval, df_genreD_eval])\n",
        "df_eval = df_eval.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "torch.save(df_train, 'df_train.pt')\n",
        "torch.save(df_eval, 'df_eval.pt')"
      ],
      "metadata": {
        "id": "Oppo9OUI0pUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"phi-1_5\", local_files_only = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "df_train = torch.load('df_train.pt')\n",
        "df_eval = torch.load('df_eval.pt')\n",
        "\n",
        "class Books3(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.text = []\n",
        "        for i in range(len(df)):\n",
        "            self.text.append(df[\"text\"][i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.text[item]\n",
        "\n",
        "dataset_train = Books3(df_train)\n",
        "dataset_eval = Books3(df_eval)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, return_tensors = \"pt\", mlm = False)"
      ],
      "metadata": {
        "id": "_t8M8DHMD1LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FINE-TUNING"
      ],
      "metadata": {
        "id": "qmV1RBNTJoi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"9994\"\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"allgather_partitions\": True,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": True\n",
        "    },\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"gradient_accumulation_steps\": \"auto\"\n",
        "}"
      ],
      "metadata": {
        "id": "1AqskrE5Yrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PhiForCausalLM.from_pretrained(\"phi-1_5\", local_files_only = True)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "estimate_zero2_model_states_mem_needs_all_live(model, num_gpus_per_node = 1, num_nodes = 1)"
      ],
      "metadata": {
        "id": "GODiFBduJmkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(output_dir = \"model\",\n",
        "                         overwrite_output_dir = True,\n",
        "                         per_device_train_batch_size = 4,\n",
        "                         per_device_eval_batch_size = 1,\n",
        "                         gradient_accumulation_steps = 1,\n",
        "                         num_train_epochs = 1,\n",
        "                         optim = \"adamw_torch\",\n",
        "                         learning_rate= 0.00001,\n",
        "                         warmup_ratio = 0.1,\n",
        "                         lr_scheduler_type = \"linear\",\n",
        "                         fp16 = True,\n",
        "                         deepspeed = ds_config,\n",
        "                         save_strategy = \"no\",\n",
        "                         logging_strategy = \"epoch\",\n",
        "                         evaluation_strategy = \"epoch\")\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  tokenizer = tokenizer,\n",
        "                  args = args,\n",
        "                  train_dataset = dataset_train,\n",
        "                  eval_dataset = dataset_eval,\n",
        "                  data_collator = data_collator)"
      ],
      "metadata": {
        "id": "go353dQe6Drs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "U6Fw166_6Fvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE"
      ],
      "metadata": {
        "id": "Ig7UeRaWJvmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"model\", local_files_only = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PhiForCausalLM.from_pretrained(\"model\", local_files_only = True)\n",
        "model = model.to(torch.device(\"cuda\"))"
      ],
      "metadata": {
        "id": "3xhIak9CJxrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"In 2067, humanity faces extinction due to a global blight. Joseph Cooper, a former NASA test pilot, along with his son and daughter, Tom and Murph, and father-in-law Donald, toil as subsistence farmers. One evening during a dust storm,\", return_tensors = \"pt\")\n",
        "inputs = inputs.to(torch.device(\"cuda\"))\n",
        "\n",
        "outputs = model.generate(inputs, max_new_tokens = 50, do_sample = False)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "41tuufSkJ1Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d65c50b-0933-427f-cf08-22a340af9704"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 2067, humanity faces extinction due to a global blight. Joseph Cooper, a former NASA test pilot, along with his son and daughter, Tom and Murph, and father-in-law Donald, toil as subsistence farmers. One evening during a dust storm, they are forced to flee their farm and seek refuge in a nearby town.\n",
            "\n",
            "The town is a small, sleepy place, with a population of only a few thousand. The town is surrounded by a vast desert, and the only source of water\n"
          ]
        }
      ]
    }
  ]
}